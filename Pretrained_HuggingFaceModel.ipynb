{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8213ce8-ad41-4ce0-bf56-79559d83bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, TFViTModel\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c51063aa-a380-4843-ba0d-8f8c7af77295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"angry\",\"happy\",\"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e25b6f9-d221-4540-80b3-a758c1e1c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_layer = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Rescaling(scale=1./255)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c7605b9-420f-4639-9774-498bce79aa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape :(224, 224, 3)\n",
      "image.shape :(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#image = cv2.imread(\"C:\\\\ajay_ml\\\\Human Emotions Detection\\\\train\\\\angry\\\\82858.jpg\") - result angry\n",
    "image = cv2.imread(\"C:\\\\ajay_ml\\\\Human Emotions Detection\\\\train\\\\sad\\\\4998.jpg\") #- result angry\n",
    "image = cv2.resize(image,(224,224))\n",
    "print('image.shape :{}'.format(image.shape))\n",
    "image = resize_rescale_layer(image)\n",
    "print('image.shape :{}'.format(image.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "031876ce-321c-4ed2-9d34-ef788e121205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "All PyTorch model weights were used when initializing TFViTModel.\n",
      "\n",
      "All the weights of TFViTModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1577a0db-bd65-442d-9aee-4b165b725d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 197, 768)\n"
     ]
    }
   ],
   "source": [
    "inputs = image_processor(image, return_tensors=\"tf\",do_rescale=False)\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)\n",
    "print(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73904956-1df9-485c-8381-c6270aca880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n"
     ]
    }
   ],
   "source": [
    "x = last_hidden_states[:,0,:]\n",
    "x = keras.layers.Dense(units=3,activation=\"softmax\")(x)\n",
    "\n",
    "predicted_label = int(tf.math.argmax(x, axis=-1))\n",
    "print(class_names[predicted_label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
